vocab_size: 50257
max_seq_length: 1024
emb_dim: 768
n_heads: 12
n_layers: 12
drop_rate: 0.1
qkv_bias: False
rope: False
attention: "nsa"  # "mha" for MultiHeadAttention, "nsa" for NativeSparseAttention
compression_block_size: 16
compression_stride: 16
selection_block_size: 8
selection_top_k: 2
window_size: 256
n_kv_heads: 12
num_epochs: 50
initial_lr: 0.0001
weight_decay: 0.1
train_data: wiki_text