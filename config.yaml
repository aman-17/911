vocab_size: 50257
max_seq_length: 2048
emb_dim: 768
n_heads: 12
n_layers: 12
drop_rate: 0.1
qkv_bias: False
rope: False
attention: "mha"
compression_block_size: 16
compression_stride: 16
selection_block_size: 8
selection_top_k: 2
window_size: 256
n_kv_heads: 12
num_epochs: 50
initial_lr: 0.0001
weight_decay: 0.1
train_data: wiki_text
q_lora_rank: null
kv_lora_rank: 384  #emb_dim // 2
qk_rope_head_dim: 64
qk_nope_head_dim: 0  #(emb_dim // n_heads)-qk_rope_head_dim
v_head_dim: 64  #emb_dim // n_heads
rope_theta: 10000.0
softcap: null
query_pre_attn_scalar: 1.0