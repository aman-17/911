vocab_size: 50257
max_seq_length: 2048
emb_dim: 768
n_heads: 12
n_layers: 12
hidden_dim: 3072
drop_rate: 0.1
qkv_bias: False
rope: True
dtype: fp32
attention: "mha"
use_flash_attn: True
compression_block_size: 16
compression_stride: 16
selection_block_size: 8
selection_top_k: 2
window_size: 256
n_kv_heads: 12
num_epochs: 50
initial_lr: 0.0001
weight_decay: 0.1
train_data: edu_fineweb10B
batch_size: 4
stride: 512
q_lora_rank: null
kv_lora_rank: 384  #emb_dim // 2
qk_rope_head_dim: 32
qk_nope_head_dim: 32  #(emb_dim // n_heads)-qk_rope_head_dimm
v_head_dim: 64  #emb_dim // n_heads
rope_theta: 10000.0
softcap: null
query_pre_attn_scalar: 1.0