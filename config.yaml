vocab_size: 50257
max_seq_length: 2048

model_arch: "llama"
emb_dim: 2048
n_heads: 32
n_layers: 16
hidden_dim: 3072

train_data: edu_fineweb10B
batch_size: 4
stride: 1024
num_workers: 1
pin_memory: true
shuffle_buffer_size: 1000

drop_rate: 0.1
qkv_bias: False
rope: True
dtype: fp32
tie_embeddings: False
attention: "mla"
use_flash_attn: True

compression_block_size: 16
compression_stride: 16
selection_block_size: 8
selection_top_k: 2
window_size: 256
n_kv_heads: 16
num_epochs: 50
initial_lr: 0.0001
weight_decay: 0.1

q_lora_rank: null
kv_lora_rank: 1024  #emb_dim // 2
qk_rope_head_dim: 128
qk_nope_head_dim: 128  #(emb_dim // n_heads)-qk_rope_head_dimm
v_head_dim: 128  #emb_dim // n_heads
rope_theta: 10000.0
softcap: null
query_pre_attn_scalar: 1.0